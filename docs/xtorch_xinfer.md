This is the critical architectural decision. In a high-end defense stack, **Training (`xTorch`)** and **Inference (`xInfer`)** live in completely different parts of the ecosystem.

Here is exactly where they fit into the 5-section Monorepo:

---

### **1. `aegis-core` (The Flight Software)**
**Primary User of: `xInfer`**

This is the embedded system on the drone/pod. It has limited compute (Jetson Orin) and needs to run in milliseconds.

*   **Where:** `aegis-sky/core/src/services/perception/`
*   **Usage:**
    *   Loads the serialized engine (`.plan` or `.engine` file) created by `xInfer`.
    *   Feeds the Fused Tensor (Radar+Vision) into the engine.
    *   Gets Bounding Boxes and Class IDs out.
*   **Why not xTorch?** `xTorch` includes the "Backward Pass" (gradients), which consumes 3x more memory. You don't need to calculate gradients in mid-air. You only need the result.

### **2. `aegis-brain` (The AI Factory)**
**Primary User of: `xTorch`**

This is the new section we discussed. It runs on a massive Server Farm (e.g., 8x H100 GPUs), not on the drone.

*   **Where:** `aegis-sky/brain/src/training/`
*   **Usage:**
    *   **Data Ingestion:** Reads the logs generated by `aegis-sim`.
    *   **Model Definition:** Uses `xtorch::nn::Module` to define the "Aura" architecture.
    *   **Training Loop:** Uses `xtorch::optim::AdamW` to update weights based on the simulated data.
*   **Where:** `aegis-sky/brain/src/export/`
*   **Usage:**
    *   Uses **`xInfer::Builder`**.
    *   Once training is done, this module takes the `xTorch` weights and compiles them into a hyper-optimized `xInfer` engine for the Jetson.

### **3. `aegis-cloud` (Fleet Forensics)**
**Secondary User of: `xInfer` (Server Side)**

This runs on the cloud backend.

*   **Where:** `aegis-sky/cloud/src/forensics/`
*   **Usage:**
    *   When a pod reports a "Kill," it uploads the sensor logs.
    *   The Cloud re-runs the detection using `xInfer` (perhaps a larger, more accurate version of the model) to verify the kill was valid ("Post-Mission Analysis").

---

### **The Code Integration Map**

Here is how your `CMakeLists.txt` files will look in the different sections:

#### **A. Inside `aegis-core/CMakeLists.txt`**
```cmake
# The embedded system ONLY needs to run models, not train them.
find_package(xInfer REQUIRED) 

add_library(perception_service SHARED src/services/perception/InferenceMgr.cpp)
target_link_libraries(perception_service PRIVATE xInfer::Runtime)
```

#### **B. Inside `aegis-brain/CMakeLists.txt`**
```cmake
# The Training Studio needs EVERYTHING.
find_package(xTorch REQUIRED)
find_package(xInfer REQUIRED)

# 1. The Trainer
add_executable(train_aura src/pipelines/TrainLoop.cpp)
target_link_libraries(train_aura PRIVATE xTorch::Core)

# 2. The Exporter (Converts xTorch weights -> xInfer Engine)
add_executable(export_model src/export/Compiler.cpp)
target_link_libraries(export_model PRIVATE xTorch::Core xInfer::Builder)
```

---

### **Visualizing the Flow**

1.  **`aegis-sim`** generates raw training data (Radar + Images).
2.  **`aegis-brain`** uses **`xTorch`** to read that data and train a `.pth` (weights) file.
3.  **`aegis-brain`** uses **`xInfer`** to compile that `.pth` file into a `.plan` (engine) file.
4.  **`aegis-core`** downloads the `.plan` file and runs it using **`xInfer`** to shoot down drones.

### **Exceptions: Online Learning**

*Can `aegis-core` use `xTorch`?*
**Yes, but only in advanced scenarios.**
If you want the drone to "Learn while flying" (e.g., it sees a new type of jammer and adapts its filters instantly), you would link `xTorch` into `aegis-core`.
*   **Pros:** Adapts to unknown threats.
*   **Cons:** Uses massive RAM and battery.
*   **Recommendation:** Do not do this for MVP. Stick to the flow above.